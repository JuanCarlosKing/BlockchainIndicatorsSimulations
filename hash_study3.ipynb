{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import datetime\n",
    "import warnings\n",
    "import sklearn\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import precision_score, accuracy_score, recall_score, f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV,RandomizedSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas_ta as ta\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import numpy as np\n",
    "import math\n",
    "import statistics\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas import read_csv\n",
    "import urllib.request, json\n",
    "from pandas import json_normalize\n",
    "import math\n",
    "Â # The first thing we want to do is import the Pandas library and set the filepath to our data file\n",
    "import pandas as pd\n",
    "import pandas_ta as ta\n",
    "import warnings\n",
    "from pandas.tseries.offsets import DateOffset\n",
    "import warnings\n",
    "import sklearn\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "from datetime import datetime, date, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from itertools import groupby, chain\n",
    "from collections import OrderedDict\n",
    "\n",
    "import warnings\n",
    "from pandas.core.common import SettingWithCopyWarning\n",
    "warnings.simplefilter(action=\"ignore\", category=SettingWithCopyWarning)\n",
    "\n",
    "import quandl\n",
    "quandl.ApiConfig.api_key = 'soxd-469x3Zp4ib_4uzc'\n",
    "\n",
    "\n",
    "##########################\n",
    "\n",
    "def correlation_study(df_close, df, date_column, value_column, close_column):\n",
    "    # join both datasets together\n",
    "    together = pd.merge(df_close[[date_column, close_column]],\n",
    "                        df[[date_column, value_column]],\n",
    "                        on= [date_column], how='left')\n",
    "    # last valid observation forward\n",
    "    together = together.fillna(method='ffill')\n",
    "    # drop NAs\n",
    "    together = together.dropna(axis=0)\n",
    "    return together.corr()[value_column].iloc[0]\n",
    "\n",
    "df_close = df_MKPRU.copy()\n",
    "df_close = df_close.reset_index()\n",
    "df_close = df_close.rename(columns={'Value': 'Close'})\n",
    "# PARAMETERS #\n",
    "date_column = 'Date'\n",
    "close_column = 'Close'\n",
    "value_column = 'Value'\n",
    "list_of_hash_df = [df_CPTRA, df_DIFF, df_ETRAV, df_ETRVU, df_HRATE, df_MIREV, df_MKPRU, df_MKTCP, df_MWNTD, \\\n",
    "                   df_MWNUS,  df_NADDU, df_NTRAN, df_NTRAT, df_NTRBL, df_NTREP, df_TOTBC, df_TOUTV, df_TRFEE, \\\n",
    "                   df_TRFUS, df_TRVOU, df_AVBLS, df_BLCHS, df_ATRCT, df_MIREV]\n",
    "list_of_hash_names= [ 'df_CPTRA', 'df_DIFF', 'df_ETRAV', 'df_ETRVU', 'df_HRATE','df_MIREV', 'df_MKPRU', \\\n",
    "                     'df_MKTCP', 'df_MWNTD', 'df_MWNUS', 'df_NADDU', 'df_NTRAN', 'df_NTRAT', 'df_NTRBL', \\\n",
    "                     'df_NTREP', 'df_TOTBC', 'df_TOUTV', 'df_TRFEE', 'df_TRFUS', 'df_TRVOU', 'df_AVBLS', 'df_BLCHS', \\\n",
    "                     'df_ATRCT', 'df_MIREV']\n",
    "zipped_list = list(zip(list_of_hash_names, list_of_hash_df))\n",
    "\n",
    "# CORRELATIONS ITERATION #\n",
    "list_of_correlations = []\n",
    "for i, j in zipped_list:\n",
    "    df_hash_features = j.copy()\n",
    "    df_hash_features = df_hash_features.reset_index()\n",
    "    corr = correlation_study(df_close, df_hash_features, date_column, value_column, close_column)\n",
    "    print(i, corr)\n",
    "\n",
    "\n",
    "##########################\n",
    "\n",
    "def get_data_yfinance(asset, interval='1d', start='2011-01-01'):\n",
    "    date_column = 'Date'\n",
    "    list_of_days = ['1d', '5d', '1wk', '1mo', '3mo']\n",
    "    list_of_minutes = ['1m', '2m', '5m', '15m', '30m', '60m', '90m']\n",
    "    df = yf.download(asset, start=start, threads= False, interval=interval)\n",
    "    df = df.reset_index()\n",
    "    if interval== '1h':\n",
    "        df = df.rename(columns={'index':'Date'})\n",
    "    elif interval in list_of_minutes:\n",
    "        df = df.rename(columns={'Datetime':'Date'})\n",
    "        #df[date_column] = df[date_column].str[:]    \n",
    "    # Now that we have loaded our data into the dataframe, we can preview it using the print & .head() function\n",
    "    df[date_column] = pd.to_datetime(df[date_column])\n",
    "    df = df.sort_values([date_column], ascending=[True]).reset_index(drop=True).sort_values([date_column], ascending=[False])\n",
    "    df = df.rename(columns={'Date':'date','Open':'open', 'High':'high', 'Low': 'low','Close':'close', 'Volume':'tradecount'})\n",
    "    return(df)\n",
    "\n",
    "# Create a Dataframe with dates\n",
    "first_date = '2009-01-02'\n",
    "first_date_obj = datetime.strptime(first_date, '%Y-%m-%d')\n",
    "today_obj = datetime.today()\n",
    "difference = today_obj - first_date_obj\n",
    "total_days = difference.days\n",
    "list_of_dates = []\n",
    "for i in range(total_days):\n",
    "    new_day = first_date_obj + timedelta(days=i)\n",
    "    new_day = new_day.strftime('%Y-%m-%d')\n",
    "    list_of_dates.append(new_day)\n",
    "df_dates = pd.DataFrame(list_of_dates, columns=['date'])\n",
    "df_dates['date'] = pd.to_datetime(df_dates['date'], format='%Y-%m-%d')\n",
    "\n",
    "df_close_yf = get_data_yfinance('BTC-USD')\n",
    "df_close_yf = df_close_yf[['date','close']]\n",
    "df_MKPRU_quandul = df_MKPRU.reset_index().rename(columns={'Date':'date', 'Value':'close'})\n",
    "df_close_clean = pd.merge(df_dates,df_MKPRU_quandul, on='date', how='left')\n",
    "df_close_clean = pd.merge(df_close_clean,df_close_yf, on='date', how='left')\n",
    "df_close_clean['close'] = df_close_clean[['close_x','close_y']].apply(lambda x: x['close_x'] if(np.all(pd.notnull(x['close_x']))) else x['close_y'], axis=1)\n",
    "df_close_clean = df_close_clean[['date','close']]\n",
    "\n",
    "\n",
    "##########################\n",
    "\n",
    "\n",
    "class target:\n",
    "    def __init__(self,df,column_date='date'):\n",
    "        self.df = df\n",
    "        self.column_date = column_date\n",
    "       \n",
    "    def max_min(self, initial_days=0, days=10, column_target='high', method=max):\n",
    "        df = self.df\n",
    "        column_date = self.column_date\n",
    "\n",
    "        df_target = df[[column_date,column_target]].copy()\n",
    "        df_target_original = df_target.copy()\n",
    "        list_of_targets=[column_date]\n",
    "        #in loop add one column per day\n",
    "        for i in range(initial_days,days+1):\n",
    "            df_target_temp = df_target_original.copy()\n",
    "            df_target_temp[column_date+str(i)] = df_target_temp[column_date] + pd.Timedelta(days=-i)\n",
    "            df_target_temp = df_target_temp[[column_date+str(i),column_target]].rename(columns={column_target:column_target+str(i)})\n",
    "            df_target=pd.merge(df_target,df_target_temp, how='left',left_on=column_date, right_on=column_date+str(i))\n",
    "            df_target = df_target.dropna()\n",
    "            list_of_targets.append(column_target+str(i))\n",
    "       \n",
    "        df_target = df_target[list_of_targets]\n",
    "        if method==max:\n",
    "            df_target['target'] = df_target[list_of_targets].max(axis=1)\n",
    "        if method==min:\n",
    "            df_target['target'] = df_target[list_of_targets].min(axis=1)\n",
    "        df_target=df_target[[column_date, 'target']]\n",
    "        df_target = df_target.sort_values([column_date], ascending=[True]).reset_index(drop=True).sort_values([column_date], ascending=[False])\n",
    "       \n",
    "        return(df_target)  \n",
    "   \n",
    "    def close(self, days=10,column_target='close'):\n",
    "        df = self.df\n",
    "        column_date = self.column_date    \n",
    "        df_target = df[[column_date,column_target]].copy()\n",
    "        df_target[column_date+ \"_-_\" +str(days)] = df_target[column_date] + pd.Timedelta(days=-days)\n",
    "        df_target = df_target.drop(columns=column_date, axis=1)\n",
    "        dictionary={column_date+ \"_-_\" +str(days):column_date, column_target:'target'}\n",
    "        df_target = df_target.rename(columns=dictionary)\n",
    "        df_target = df_target[[column_date, 'target']]\n",
    "        return (df_target)\n",
    "   \n",
    "    def direction(self,days=10,column_target='close', column_high='high', column_low='low'):\n",
    "        df = self.df\n",
    "        column_date = self.column_date\n",
    "        df_close = df[[column_date, column_target]].copy()\n",
    "        df_target_max = target(df).max_min(method=max, column_target=column_high).rename(columns={'target':'max'})\n",
    "        df_target_min = target(df).max_min(method=min, column_target=column_low).rename(columns={'target':'min'})\n",
    "        df_target_close = target(df).close(column_target=column_target).rename(columns={'target':'target_close'})\n",
    "        df_final = pd.merge(df_close, df_target_max, on=column_date, how='inner')\n",
    "        df_final = pd.merge(df_final, df_target_min, on=column_date, how='inner')\n",
    "        df_final = pd.merge(df_final, df_target_close, on=column_date, how='inner')\n",
    "        df_final['direction'] =df_final[column_target] + (df_final['max'] - df_final[column_target]  + df_final['min'] - df_final[column_target] + df_final['target_close'] - df_final[column_target])/3\n",
    "        df_final = df_final[['date','direction']]\n",
    "        return(df_final)\n",
    "\n",
    "df = df_close_clean.copy()\n",
    "\n",
    "df_direction = target(df).direction(days=7, column_target='close', column_high='close', column_low='close')\n",
    "df = pd.merge(df,df_direction, on='date', how='left')\n",
    "df['difference'] = (df['direction'] - df['close'])/df['close']\n",
    "df['target_class']=df[\"difference\"].apply(lambda x: 1 if x>0. else 0)\n",
    "#df = df[['date','open','high','low','close', 'target_class']]\n",
    "df_target = df[['date', 'close','target_class']]\n",
    "\n",
    "\n",
    "##########################\n",
    "\n",
    "# DATA PREPARATION\n",
    "\n",
    "# To start again, creation of unique df\n",
    "list_of_fetures_df = [\n",
    "df_MWNUS,\\\n",
    "df_HRATE,\\\n",
    "df_DIFF,\\\n",
    "df_MKPRU,\\\n",
    "df_MIREV,\\\n",
    "df_CPTRA,\\\n",
    "df_TRVOU,\\\n",
    "df_ETRVU,\\\n",
    "df_ETRAV,\\\n",
    "df_TOUTV,\\\n",
    "df_NTRBL,\\\n",
    "df_NTRAT,\\\n",
    "df_NADDU,\\\n",
    "df_NTREP,\\\n",
    "df_NTRAN,\\\n",
    "df_TRFUS,\\\n",
    "df_TRFEE,\\\n",
    "df_MKTCP,\\\n",
    "df_TOTBC,\\\n",
    "df_AVBLS,\\\n",
    "df_BLCHS,\\\n",
    "df_ATRCT]\n",
    "\n",
    "list_of_names_df = ['df_MWNUS',\\\n",
    "'df_HRATE',\\\n",
    "'df_DIFF',\\\n",
    "'df_MKPRU',\\\n",
    "'df_MIREV',\\\n",
    "'df_CPTRA',\\\n",
    "'df_TRVOU',\\\n",
    "'df_ETRVU',\\\n",
    "'df_ETRAV',\\\n",
    "'df_TOUTV',\\\n",
    "'df_NTRBL',\\\n",
    "'df_NTRAT',\\\n",
    "'df_NADDU',\\\n",
    "'df_NTREP',\\\n",
    "'df_NTRAN',\\\n",
    "'df_TRFUS',\\\n",
    "'df_TRFEE',\\\n",
    "'df_MKTCP',\\\n",
    "'df_TOTBC',\\\n",
    "'df_AVBLS',\\\n",
    "'df_BLCHS',\\\n",
    "'df_ATRCT']\n",
    "\n",
    "df_final = df_target.copy()\n",
    "zipped_corr_list = list(zip(list_of_fetures_df, list_of_names_df))\n",
    "for i, j in zipped_corr_list:\n",
    "    i = i.reset_index().rename(columns={'Date':'date', 'Value':j})\n",
    "    df_final = pd.merge(df_final,i, on='date', how='left')\n",
    "#df_rf = df_final.drop(columns=['date'], axis=1)\n",
    "df_MWNTD = df_MWNTD.reset_index().rename(columns={'Date':'date', 'Value':'df_MWNTD'})\n",
    "df_MWNTD['date'] = pd.DatetimeIndex(df_MWNTD.date) + pd.DateOffset(1)\n",
    "#df_MWTRV = df_MWTRV.reset_index().rename(columns={'Date':'date', 'Value':'df_MWTRV'})\n",
    "#df_MWTRV['date'] = pd.DatetimeIndex(df_MWTRV.date) + pd.DateOffset(1)\n",
    "df_final = pd.merge(df_final,df_MWNTD, on='date', how='left')\n",
    "#df_final = pd.merge(df_final,df_MWTRV, on='date', how='inner')\n",
    "#df_final = df_final.drop(columns='index')\n",
    "list_of_fetures_df = [\n",
    "df_MWNUS,\\\n",
    "df_HRATE,\\\n",
    "df_DIFF,\\\n",
    "df_MKPRU,\\\n",
    "df_MIREV,\\\n",
    "df_CPTRA,\\\n",
    "df_TRVOU,\\\n",
    "df_ETRVU,\\\n",
    "df_ETRAV,\\\n",
    "df_TOUTV,\\\n",
    "df_NTRBL,\\\n",
    "df_NTRAT,\\\n",
    "df_NADDU,\\\n",
    "df_NTREP,\\\n",
    "df_NTRAN,\\\n",
    "df_TRFUS,\\\n",
    "df_TRFEE,\\\n",
    "df_MKTCP,\\\n",
    "df_TOTBC,\\\n",
    "df_AVBLS,\\\n",
    "df_BLCHS,\\\n",
    "df_ATRCT,\\\n",
    "df_MWNTD]\n",
    "\n",
    "list_of_names_df = ['df_MWNUS',\\\n",
    "'df_HRATE',\\\n",
    "'df_DIFF',\\\n",
    "'df_MKPRU',\\\n",
    "'df_MIREV',\\\n",
    "'df_CPTRA',\\\n",
    "'df_TRVOU',\\\n",
    "'df_ETRVU',\\\n",
    "'df_ETRAV',\\\n",
    "'df_TOUTV',\\\n",
    "'df_NTRBL',\\\n",
    "'df_NTRAT',\\\n",
    "'df_NADDU',\\\n",
    "'df_NTREP',\\\n",
    "'df_NTRAN',\\\n",
    "'df_TRFUS',\\\n",
    "'df_TRFEE',\\\n",
    "'df_MKTCP',\\\n",
    "'df_TOTBC',\\\n",
    "'df_AVBLS',\\\n",
    "'df_BLCHS',\\\n",
    "'df_ATRCT',\\\n",
    "'df_MWNTD']\n",
    "\n",
    "def calculate_sublists(list_of_nulls):\n",
    "    iterator = 0\n",
    "    list_of_lists_of_index = []\n",
    "    list_small_index = []\n",
    "    check = list_of_nulls[0] - iterator\n",
    "    for i in range(len(list_of_nulls)):\n",
    "        value = list_of_nulls[i]\n",
    "        if check != value-iterator:\n",
    "            list_of_lists_of_index.append(list_small_index)\n",
    "            list_small_index = []\n",
    "            iterator=0\n",
    "            check = list_of_nulls[i]\n",
    "            list_small_index.append(value)\n",
    "        else:\n",
    "            list_small_index.append(value)\n",
    "        iterator+=1\n",
    "    list_of_lists_of_index.append(list_small_index)\n",
    "    return(list_of_lists_of_index)\n",
    "\n",
    "def add_average_to_index_list(df_in, field, list_of_nulls):\n",
    "    df = df_in.copy()\n",
    "    list_of_lists = calculate_sublists(list_of_nulls)\n",
    "    for list_ in list_of_lists:\n",
    "        if list_[0] == 0:\n",
    "            #take the last_value\n",
    "            average = df[[field]].iloc[list_[-1]+1]\n",
    "        else:\n",
    "            average = (df[[field]].iloc[list_[0]-1] + df[[field]].iloc[list_[-1]+1]) / 2\n",
    "        for i in list_:\n",
    "            df.iloc[i, df.columns.get_loc(field)] = average[0]\n",
    "    return df\n",
    "\n",
    "df_cleaned = df_final.copy()\n",
    "for feature in list_of_names_df:\n",
    "    list_nulos=df_cleaned[df_cleaned[feature].isna()].index\n",
    "    if len(list_nulos) > 0:\n",
    "        df_cleaned = add_average_to_index_list(df_cleaned, feature, list_nulos)\n",
    "    else:\n",
    "        print(\"probblem with \", feature)\n",
    "df_final=df_cleaned.copy()\n",
    "\n",
    "\n",
    "##########################\n",
    "\n",
    "# obtain ribbons\n",
    "list_of_sma = []\n",
    "list_of_ribbon = []\n",
    "for i in list_of_names_df:\n",
    "    sma_5 = i + '_sma_5'\n",
    "    list_of_sma.append(sma_5)\n",
    "    df_final[sma_5] = pd.Series.to_frame(ta.sma(df_final[i], length=5))\n",
    "    sma_10 = i + '_sma_10'\n",
    "    list_of_sma.append(sma_10)\n",
    "    df_final[sma_10] = pd.Series.to_frame(ta.sma(df_final[i], length=10))\n",
    "    sma_20 = i + '_sma_20'\n",
    "    list_of_sma.append(sma_20)\n",
    "    df_final[sma_20] = pd.Series.to_frame(ta.sma(df_final[i], length=20))\n",
    "    sma_30 = i + '_sma_30'\n",
    "    list_of_sma.append(sma_30)\n",
    "    df_final[sma_30] = pd.Series.to_frame(ta.sma(df_final[i], length=30))\n",
    "    sma_40 = i + '_sma_40'\n",
    "    list_of_sma.append(sma_40)\n",
    "    df_final[sma_40] = pd.Series.to_frame(ta.sma(df_final[i], length=40))\n",
    "    sma_60 = i + '_sma_60'\n",
    "    list_of_sma.append(sma_60)\n",
    "    df_final[sma_60] = pd.Series.to_frame(ta.sma(df_final[i], length=60))\n",
    "    ribbon_5_10 = 'ribbon_' + i + '_5_10'\n",
    "    list_of_ribbon.append(ribbon_5_10)\n",
    "    df_final[ribbon_5_10] = (df_final[sma_5] - df_final[sma_10])/df_final[sma_10]\n",
    "    ribbon_10_20 = 'ribbon_' + i + '_10_20'\n",
    "    list_of_ribbon.append(ribbon_10_20)\n",
    "    df_final[ribbon_10_20] = (df_final[sma_10] - df_final[sma_20])/df_final[sma_20]\n",
    "    ribbon_20_40 = 'ribbon_' + i + '_20_40'\n",
    "    list_of_ribbon.append(ribbon_20_40)\n",
    "    df_final[ribbon_20_40] = (df_final[sma_20] - df_final[sma_40])/df_final[sma_40]\n",
    "    ribbon_30_60 = 'ribbon_' + i + '_30_60'\n",
    "    list_of_ribbon.append(ribbon_30_60)\n",
    "    df_final[ribbon_30_60] = (df_final[sma_30] - df_final[sma_60])/df_final[sma_60]\n",
    "\n",
    "list_of_date = ['date']\n",
    "list_of_sma_to_calculate_inc = list_of_date + list_of_sma\n",
    "list_of_ribbon_to_calculate_inc = list_of_date + list_of_ribbon\n",
    "df_sma_to_calculate_inc = df_final[list_of_sma_to_calculate_inc].copy()\n",
    "df_ribbon_to_calculate_inc = df_final[list_of_ribbon_to_calculate_inc].copy()\n",
    "df_final.to_csv('datos_hash.csv', encoding='Latin', index=False, sep=';', decimal=',')\n",
    "\n",
    "# Create previous data\n",
    "def creation_df_before(df_final, days, date_column='date', list_of_columns_to_delete=['target_class', 'close']):\n",
    "    df_final_before = df_final.copy()\n",
    "    df_final_before = df_final_before.drop(columns=list_of_columns_to_delete)\n",
    "    df_final_before[date_column] = pd.DatetimeIndex(df_final_before.date) + pd.DateOffset(days)\n",
    "    dictionary_change_columns = {}\n",
    "    list_columns_df_final_before = []\n",
    "    for i in df_final_before.columns:\n",
    "        dictionary_change_columns[i] = 'before_1_day_' + str(i)\n",
    "        list_columns_df_final_before.append('before_1_day_' + str(i))\n",
    "    df_final_before = df_final_before.rename(columns=dictionary_change_columns)\n",
    "    return df_final_before\n",
    "\n",
    "def incremental_calculation(df, *args, days=1, date_column= 'date', columns_to_drop=['target_class', 'close']):\n",
    "    df_final_before = creation_df_before(df_final, days)\n",
    "    df_final_before = df.copy()\n",
    "    df_final_before = df_final_before.drop(columns=columns_to_drop)\n",
    "    df_final_before[date_column] = pd.DatetimeIndex(df_final_before.date) + pd.DateOffset(days)\n",
    "    dictionary_change_columns = {}\n",
    "    if len(args) == 0:\n",
    "        list_columns_df_final_before = []\n",
    "        for i in df_final_before.columns:\n",
    "            dictionary_change_columns[i] = 'before_' + str(days) + '_days_' + str(i)\n",
    "            if i != date_column:\n",
    "                list_columns_df_final_before.append('before_' + str(days) + '_days_' + str(i))\n",
    "        df_final_before = df_final_before.rename(columns=dictionary_change_columns)\n",
    "        df_final_before = df_final_before.rename(columns={'before_' + str(days) + '_days_' + 'date': 'date'})\n",
    "    else:\n",
    "        list_columns_df_final_before = []\n",
    "        for n in args:\n",
    "            list_columns_df_final_before.append(n)\n",
    "        df_final_before = df_final_before[list_columns_df_final_before]\n",
    "        for i in df_final_before.columns:\n",
    "            dictionary_change_columns[i] = 'before_' + str(days) + '_days_' + str(i)\n",
    "            if i != date_column:\n",
    "                list_columns_df_final_before.append('before_' + str(days) + '_days_' + str(i))\n",
    "        df_final_before = df_final_before.rename(columns=dictionary_change_columns)\n",
    "        df_final_before = df_final_before.rename(columns={'before_' + str(days) + '_days_' + 'date': 'date'})\n",
    "    return(df_final_before, list_columns_df_final_before)\n",
    "\n",
    "df_sma_before_1_days,list_columns_df_sma_before_1_days = incremental_calculation(df_sma_to_calculate_inc, days=1, columns_to_drop=[])\n",
    "df_sma_before_4_days,list_columns_df_sma_before_4_days = incremental_calculation(df_sma_to_calculate_inc, days=4, columns_to_drop=[])\n",
    "df_sma_before_10_days,list_columns_df_sma_before_10_days = incremental_calculation(df_sma_to_calculate_inc, days=10, columns_to_drop=[])\n",
    "df_ribbon_before_1_days,list_columns_df_ribbon_before_1_days = incremental_calculation(df_ribbon_to_calculate_inc, days=1, columns_to_drop=[])\n",
    "df_ribbon_before_4_days,list_columns_df_ribbon_before_4_days = incremental_calculation(df_ribbon_to_calculate_inc, days=4, columns_to_drop=[])\n",
    "df_ribbon_before_10_days,list_columns_df_ribbon_before_10_days = incremental_calculation(df_ribbon_to_calculate_inc, days=10, columns_to_drop=[])\n",
    "\n",
    "df_final = pd.merge(df_final, df_sma_before_1_days, on='date', how='left')\n",
    "df_final = pd.merge(df_final, df_sma_before_4_days, on='date', how='left')\n",
    "df_final = pd.merge(df_final, df_sma_before_10_days, on='date', how='left')\n",
    "df_final = pd.merge(df_final, df_ribbon_before_1_days, on='date', how='left')\n",
    "df_final = pd.merge(df_final, df_ribbon_before_4_days, on='date', how='left')\n",
    "df_final = pd.merge(df_final, df_ribbon_before_10_days, on='date', how='left')\n",
    "\n",
    "\n",
    "##########################\n",
    "\n",
    "# Creation of incrementales\n",
    "def get_incremental_value(df_in, list_of_features, days):\n",
    "    df = df_in.copy()\n",
    "    previous = 'before_' + str(days) + '_days_'\n",
    "    inc = 'inc_' + str(days) + '_days_'\n",
    "    list_of_incremental_values = []\n",
    "    for i in list_of_features:\n",
    "        df[inc + str(i)] = 100 * (df[i] - df[previous + str(i)]) / df[previous + str(i)]\n",
    "    return df\n",
    "df_total_sma = get_incremental_value(df_final, list_of_sma, 1)\n",
    "df_total_sma = get_incremental_value(df_total_sma, list_of_sma, 4)\n",
    "df_total_sma = get_incremental_value(df_total_sma, list_of_sma, 10)\n",
    "df_total_sma_ribbon = get_incremental_value(df_total_sma, list_of_ribbon, 1)\n",
    "df_total_sma_ribbon = get_incremental_value(df_total_sma_ribbon, list_of_ribbon, 4)\n",
    "df_total_sma_ribbon = get_incremental_value(df_total_sma_ribbon, list_of_ribbon, 10)\n",
    "\n",
    "# Delete columns unuseful\n",
    "\n",
    "list_of_columns_to_delete = []\n",
    "list_of_list_to_delete = [list_of_sma, list_of_names_df, list_of_sma_to_calculate_inc, list_columns_df_sma_before_1_days, \\\n",
    "                          list_columns_df_sma_before_4_days, list_columns_df_sma_before_10_days, list_columns_df_ribbon_before_1_days, \\\n",
    "                          list_columns_df_ribbon_before_4_days, list_columns_df_ribbon_before_10_days]\n",
    "for i in list_of_list_to_delete:\n",
    "    for j in i:\n",
    "        if j != 'date':\n",
    "            list_of_columns_to_delete.append(j)\n",
    "\n",
    "df = df_total_sma_ribbon.drop(columns=list_of_columns_to_delete).dropna()\n",
    "\n",
    "\n",
    "##########################\n",
    "\n",
    "\n",
    "def sma_long_and_short_calculation(df_final, indicator, small_sma, big_sma, date_column='date', close_column='close'):\n",
    "    df_calculo = df_final[[date_column, \\\n",
    "                           'df_'+indicator, \\\n",
    "                           'df_'+ indicator +'_sma_' + str(small_sma), \\\n",
    "                           'df_'+indicator + '_sma_' + str(big_sma), \\\n",
    "                           'before_1_days_ribbon_df_' + indicator + '_'+ str(small_sma) + '_' + str(big_sma), \\\n",
    "                           'ribbon_df_' + indicator + '_' + str(small_sma) + '_' + str(big_sma), \\\n",
    "                           close_column]]\n",
    "    # 2  long y short signals calculation\n",
    "    df_calculo[\"long_signal\"] = df_calculo[['before_1_days_ribbon_df_' + indicator + '_'+ str(small_sma) + '_' + str(big_sma),\\\n",
    "                                            'ribbon_df_' + indicator + '_' + str(small_sma) + '_' + str(big_sma)\\\n",
    "                                           ]].apply(lambda x: 1 if x['before_1_days_ribbon_df_' + indicator + '_'+ str(small_sma) + '_' + str(big_sma)]<0 and x['ribbon_df_' + indicator + '_' + str(small_sma) + '_' + str(big_sma)]>=0  else 0, axis=1)\n",
    "   \n",
    "    df_calculo[\"short_signal\"] = df_calculo[['before_1_days_ribbon_df_' + indicator + '_'+ str(small_sma) + '_' + str(big_sma),\\\n",
    "                                             'ribbon_df_' + indicator + '_' + str(small_sma) + '_' + str(big_sma)]].apply(lambda x: 1 if x['before_1_days_ribbon_df_' + indicator + '_'+ str(small_sma) + '_' + str(big_sma)]>0 and x['ribbon_df_' + indicator + '_' + str(small_sma) + '_' + str(big_sma)]<=0  else 0, axis=1)\n",
    "    # 3 calculation maximum, minimum, differences\n",
    "    list_of_operations = []\n",
    "    list_of_entry_price = []\n",
    "    list_of_exit_price = []\n",
    "    list_of_date_entry=[]\n",
    "    list_of_date_exit=[]\n",
    "    list_of_maximum=[]\n",
    "    list_of_minimum=[]\n",
    "\n",
    "    maximum_value=0\n",
    "    minimum_value=0\n",
    "    entry_long_value=0\n",
    "    entry_short_value=0\n",
    "   \n",
    "    date_enter_short = df_calculo.iloc[0][date_column]\n",
    "    date_enter_long = df_calculo.iloc[0][date_column]\n",
    "   \n",
    "    for i in range(len(df_calculo)):\n",
    "        if df_calculo.iloc[i]['short_signal']== 1:\n",
    "            if df_calculo.iloc[i][close_column] < minimum_value:\n",
    "                minimum_value = df_calculo.iloc[i][close_column]\n",
    "            elif df_calculo.iloc[i][close_column] > maximum_value:\n",
    "                maximum_value = df_calculo.iloc[i][close_column]\n",
    "            #creation new values in long list\n",
    "            entry_short_value = df_calculo.iloc[i][close_column]\n",
    "            date_enter_short = df_calculo.iloc[i][date_column]\n",
    "            previous_maximum = maximum_value\n",
    "            previous_minimum = minimum_value\n",
    "            minimum_value = df_calculo.iloc[i][close_column]\n",
    "            maximum_value = df_calculo.iloc[i][close_column]\n",
    "            if entry_short_value>0:\n",
    "                list_of_operations.append('long')\n",
    "                list_of_entry_price.append(entry_long_value)\n",
    "                list_of_exit_price.append(entry_short_value)\n",
    "                list_of_date_entry.append(date_enter_long)\n",
    "                list_of_date_exit.append(date_enter_short)\n",
    "                list_of_maximum.append(previous_maximum)\n",
    "                list_of_minimum.append(previous_minimum)\n",
    "\n",
    "           \n",
    "        elif df_calculo.iloc[i]['long_signal']== 1:\n",
    "            if df_calculo.iloc[i][close_column] < minimum_value:\n",
    "                minimum_value = df_calculo.iloc[i][close_column]\n",
    "            elif df_calculo.iloc[i][close_column] > maximum_value:\n",
    "                maximum_value = df_calculo.iloc[i][close_column]\n",
    "            date_enter_long = df_calculo.iloc[i][date_column]\n",
    "            entry_long_value = df_calculo.iloc[i][close_column]\n",
    "            previous_maximum = maximum_value\n",
    "            previous_minimum = minimum_value\n",
    "            maximum_value = df_calculo.iloc[i][close_column]\n",
    "            minimum_value = df_calculo.iloc[i][close_column]\n",
    "            if entry_short_value>0:\n",
    "                list_of_operations.append('short')\n",
    "                list_of_entry_price.append(entry_short_value)\n",
    "                list_of_exit_price.append(entry_long_value)\n",
    "                list_of_date_entry.append(date_enter_short)\n",
    "                list_of_date_exit.append(date_enter_long)\n",
    "                list_of_maximum.append(previous_maximum)\n",
    "                list_of_minimum.append(previous_minimum)\n",
    "\n",
    "       \n",
    "        if df_calculo.iloc[i][close_column] < minimum_value:\n",
    "            minimum_value = df_calculo.iloc[i][close_column]\n",
    "        elif df_calculo.iloc[i][close_column] > maximum_value:\n",
    "            maximum_value = df_calculo.iloc[i][close_column]\n",
    "    data = pd.DataFrame(columns = ['indicator', 'operation', 'entry_price', 'exit_price', 'maximum_price', 'minimum_price', 'entry_date' ,'exit_date'])\n",
    "    df_out = data.append(pd.DataFrame({'indicator':indicator, 'operation':list_of_operations, 'entry_price':list_of_entry_price, 'exit_price':list_of_exit_price, 'maximum_price':list_of_maximum, 'minimum_price':list_of_minimum, 'entry_date':list_of_date_entry ,'exit_date':list_of_date_exit}), ignore_index = True)\n",
    "\n",
    "    return(df_out)\n",
    "\n",
    "\n",
    "\n",
    "##########################\n",
    "\n",
    "list_of_indicators_to_sma_sin_calculation = [\\\n",
    "'MWNUS',\\\n",
    "'HRATE',\\\n",
    "'DIFF',\\\n",
    "'MKPRU',\\\n",
    "'MIREV',\\\n",
    "'CPTRA',\\\n",
    "'TRVOU',\\\n",
    "'ETRVU',\\\n",
    "'ETRAV',\\\n",
    "'TOUTV',\\\n",
    "'NTRBL',\\\n",
    "'NTRAT',\\\n",
    "'NADDU',\\\n",
    "'NTREP',\\\n",
    "'NTRAN',\\\n",
    "'TRFUS',\\\n",
    "'TRFEE',\\\n",
    "'MKTCP',\\\n",
    "'TOTBC',\\\n",
    "'AVBLS',\\\n",
    "'BLCHS',\\\n",
    "'ATRCT']\n",
    "df_out = sma_long_and_short_calculation(df_final, 'CPTRA', 30, 60)\n",
    "\n",
    "\n",
    "##########################\n",
    "\n",
    "\n",
    "final_data = pd.DataFrame(columns = ['indicator', 'operation', 'entry_price', 'exit_price', 'maximum_price', \\\n",
    "                                     'minimum_price', 'entry_date' ,'exit_date'])\n",
    "for indicator_ in list_of_indicators_to_sma_sin_calculation:\n",
    "    new_df = sma_long_and_short_calculation(df_final, indicator_, 30, 60)\n",
    "    df_out = pd.concat([df_out,new_df],axis=0)\n",
    "df_out.to_csv('longs_and_short_signals.csv', encoding='Latin', sep=';', decimal=',')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
