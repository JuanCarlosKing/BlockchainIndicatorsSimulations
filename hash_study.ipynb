{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import datetime\n",
    "import warnings\n",
    "import sklearn\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import precision_score, accuracy_score, recall_score, f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV,RandomizedSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import numpy as np\n",
    "import math\n",
    "import statistics\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas import read_csv\n",
    "import urllib.request, json\n",
    "from pandas import json_normalize\n",
    "import math\n",
    " # The first thing we want to do is import the Pandas library and set the filepath to our data file\n",
    "import pandas as pd\n",
    "import pandas_ta as ta\n",
    "import warnings\n",
    "from pandas.tseries.offsets import DateOffset\n",
    "import warnings\n",
    "import sklearn\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from itertools import groupby, chain\n",
    "from collections import OrderedDict\n",
    "\n",
    "import quandl\n",
    "quandl.ApiConfig.api_key = 'soxd-469x3Zp4ib_4uzc'\n",
    "\n",
    "\n",
    "df_MKPRU = quandl.get(\"BCHAIN/MKPRU\", authtoken=\"soxd-469x3Zp4ib_4uzc\")\n",
    "\n",
    "df_MWNUS = quandl.get(\"BCHAIN/MWNUS\", authtoken=\"soxd-469x3Zp4ib_4uzc\")\n",
    "df_HRATE = quandl.get(\"BCHAIN/HRATE\", authtoken=\"soxd-469x3Zp4ib_4uzc\")\n",
    "df_DIFF = quandl.get(\"BCHAIN/DIFF\", authtoken=\"soxd-469x3Zp4ib_4uzc\")\n",
    "df_MKPRU = quandl.get(\"BCHAIN/MKPRU\", authtoken=\"soxd-469x3Zp4ib_4uzc\")\n",
    "df_MIREV = quandl.get(\"BCHAIN/MIREV\", authtoken=\"soxd-469x3Zp4ib_4uzc\")\n",
    "df_CPTRA = quandl.get(\"BCHAIN/CPTRA\", authtoken=\"soxd-469x3Zp4ib_4uzc\")\n",
    "df_TRVOU = quandl.get(\"BCHAIN/TRVOU\", authtoken=\"soxd-469x3Zp4ib_4uzc\")\n",
    "df_CPTRV = quandl.get(\"BCHAIN/CPTRV\", authtoken=\"soxd-469x3Zp4ib_4uzc\")\n",
    "df_ETRVU = quandl.get(\"BCHAIN/ETRVU\", authtoken=\"soxd-469x3Zp4ib_4uzc\")\n",
    "df_ETRAV = quandl.get(\"BCHAIN/ETRAV\", authtoken=\"soxd-469x3Zp4ib_4uzc\")\n",
    "df_TOUTV = quandl.get(\"BCHAIN/TOUTV\", authtoken=\"soxd-469x3Zp4ib_4uzc\")\n",
    "df_NTRBL = quandl.get(\"BCHAIN/NTRBL\", authtoken=\"soxd-469x3Zp4ib_4uzc\")\n",
    "df_NTRAT = quandl.get(\"BCHAIN/NTRAT\", authtoken=\"soxd-469x3Zp4ib_4uzc\")\n",
    "df_NADDU = quandl.get(\"BCHAIN/NADDU\", authtoken=\"soxd-469x3Zp4ib_4uzc\")\n",
    "df_NTREP = quandl.get(\"BCHAIN/NTREP\", authtoken=\"soxd-469x3Zp4ib_4uzc\")\n",
    "df_NTRAN = quandl.get(\"BCHAIN/NTRAN\", authtoken=\"soxd-469x3Zp4ib_4uzc\")\n",
    "df_TRFUS = quandl.get(\"BCHAIN/TRFUS\", authtoken=\"soxd-469x3Zp4ib_4uzc\")\n",
    "df_TRFEE = quandl.get(\"BCHAIN/TRFEE\", authtoken=\"soxd-469x3Zp4ib_4uzc\")\n",
    "df_MKTCP = quandl.get(\"BCHAIN/MKTCP\", authtoken=\"soxd-469x3Zp4ib_4uzc\")\n",
    "df_TOTBC = quandl.get(\"BCHAIN/TOTBC\", authtoken=\"soxd-469x3Zp4ib_4uzc\")\n",
    "df_MWNTD = quandl.get(\"BCHAIN/MWNTD\", authtoken=\"soxd-469x3Zp4ib_4uzc\")   # HASTA AYER\n",
    "df_MWTRV = quandl.get(\"BCHAIN/MWTRV\", authtoken=\"soxd-469x3Zp4ib_4uzc\")   # HASTA AYER\n",
    "#df_BCDDY = quandl.get(\"BCHAIN/BCDDY\", authtoken=\"soxd-469x3Zp4ib_4uzc\")\n",
    "#df_BCDDM = quandl.get(\"BCHAIN/BCDDM\", authtoken=\"soxd-469x3Zp4ib_4uzc\")\n",
    "#df_BCDDW = quandl.get(\"BCHAIN/BCDDW\", authtoken=\"soxd-469x3Zp4ib_4uzc\")\n",
    "#df_BCDDE = quandl.get(\"BCHAIN/BCDDE\", authtoken=\"soxd-469x3Zp4ib_4uzc\")\n",
    "#df_BCDDC = quandl.get(\"BCHAIN/BCDDC\", authtoken=\"soxd-469x3Zp4ib_4uzc\")\n",
    "#df_TVTVR = quandl.get(\"BCHAIN/TVTVR\", authtoken=\"soxd-469x3Zp4ib_4uzc\")\n",
    "#df_NETDF = quandl.get(\"BCHAIN/NETDF\", authtoken=\"soxd-469x3Zp4ib_4uzc\")\n",
    "#df_MIOPM = quandl.get(\"BCHAIN/MIOPM\", authtoken=\"soxd-469x3Zp4ib_4uzc\")\n",
    "\n",
    "def correlation_study(df_close, df, date_column, value_column, close_column):\n",
    "    # join both datasets together\n",
    "    together = pd.merge(df_close[[date_column, close_column]],\n",
    "                        df[[date_column, value_column]],\n",
    "                        on= [date_column], how='left')\n",
    "    # last valid observation forward\n",
    "    together = together.fillna(method='ffill')\n",
    "    # drop NAs\n",
    "    together = together.dropna(axis=0)\n",
    "    return together.corr()[value_column].iloc[0]\n",
    "df_close = df_MKPRU.copy()\n",
    "df_close = df_close.reset_index()\n",
    "df_close = df_close.rename(columns={'Value': 'Close'})\n",
    "# PARAMETERS #\n",
    "date_column = 'Date'\n",
    "close_column = 'Close'\n",
    "value_column = 'Value'\n",
    "list_of_hash_df = [df_CPTRA, df_CPTRV, df_DIFF, df_ETRAV, df_ETRVU, df_HRATE, df_MIREV, df_MKPRU, df_MKTCP, df_MWNTD, df_MWNUS, df_MWTRV, df_NADDU, df_NTRAN, df_NTRAT, df_NTRBL, df_NTREP, df_TOTBC, df_TOUTV, df_TRFEE, df_TRFUS, df_TRVOU]\n",
    "list_of_hash_names= [ 'df_CPTRA', 'df_CPTRV', 'df_DIFF', 'df_ETRAV', 'df_ETRVU', 'df_HRATE','df_MIREV', 'df_MKPRU', 'df_MKTCP', 'df_MWNTD', 'df_MWNUS', 'df_MWTRV', 'df_NADDU', 'df_NTRAN', 'df_NTRAT', 'df_NTRBL', 'df_NTREP', 'df_TOTBC', 'df_TOUTV', 'df_TRFEE', 'df_TRFUS', 'df_TRVOU']\n",
    "zipped_list = list(zip(list_of_hash_names, list_of_hash_df))\n",
    "\n",
    "# CORRELATIONS ITERATION #\n",
    "list_of_correlations = []\n",
    "for i, j in zipped_list:\n",
    "    df_hash_features = j.copy()\n",
    "    df_hash_features = df_hash_features.reset_index()\n",
    "    corr = correlation_study(df_close, df_hash_features, date_column, value_column, close_column)\n",
    "    print(i, corr)\n",
    "\n",
    "\n",
    "###################################\n",
    "\n",
    "class target:\n",
    "    def __init__(self,df,column_date='date'):\n",
    "        self.df = df\n",
    "        self.column_date = column_date\n",
    "       \n",
    "    def max_min(self, initial_days=0, days=10, column_target='high', method=max):\n",
    "        df = self.df\n",
    "        column_date = self.column_date\n",
    "\n",
    "        df_target = df[[column_date,column_target]].copy()\n",
    "        df_target_original = df_target.copy()\n",
    "        list_of_targets=[column_date]\n",
    "        #in loop add one column per day\n",
    "        for i in range(initial_days,days+1):\n",
    "            df_target_temp = df_target_original.copy()\n",
    "            df_target_temp[column_date+str(i)] = df_target_temp[column_date] + pd.Timedelta(days=-i)\n",
    "            df_target_temp = df_target_temp[[column_date+str(i),column_target]].rename(columns={column_target:column_target+str(i)})\n",
    "            df_target=pd.merge(df_target,df_target_temp, how='left',left_on=column_date, right_on=column_date+str(i))\n",
    "            df_target = df_target.dropna()\n",
    "            list_of_targets.append(column_target+str(i))\n",
    "       \n",
    "        df_target = df_target[list_of_targets]\n",
    "        if method==max:\n",
    "            df_target['target'] = df_target[list_of_targets].max(axis=1)\n",
    "        if method==min:\n",
    "            df_target['target'] = df_target[list_of_targets].min(axis=1)\n",
    "        df_target=df_target[[column_date, 'target']]\n",
    "        df_target = df_target.sort_values([column_date], ascending=[True]).reset_index(drop=True).sort_values([column_date], ascending=[False])\n",
    "       \n",
    "        return(df_target)  \n",
    "   \n",
    "    def close(self, days=10,column_target='close'):\n",
    "        df = self.df\n",
    "        column_date = self.column_date    \n",
    "        df_target = df[[column_date,column_target]].copy()\n",
    "        df_target[column_date+ \"_-_\" +str(days)] = df_target[column_date] + pd.Timedelta(days=-days)\n",
    "        df_target = df_target.drop(columns=column_date, axis=1)\n",
    "        dictionary={column_date+ \"_-_\" +str(days):column_date, column_target:'target'}\n",
    "        df_target = df_target.rename(columns=dictionary)\n",
    "        df_target = df_target[[column_date, 'target']]\n",
    "        return (df_target)\n",
    "   \n",
    "    def direction(self,days=10,column_target='close', column_high='high', column_low='low'):\n",
    "        df = self.df\n",
    "        column_date = self.column_date\n",
    "        df_close = df[[column_date, 'close']].copy()\n",
    "        df_target_max = target(df).max_min(method=max).rename(columns={'target':'max'})\n",
    "        df_target_min = target(df).max_min(method=min, column_target='low').rename(columns={'target':'min'})\n",
    "        df_target_close = target(df).close().rename(columns={'target':'target_close'})\n",
    "        df_final = pd.merge(df_close, df_target_max, on=column_date, how='inner')\n",
    "        df_final = pd.merge(df_final, df_target_min, on=column_date, how='inner')\n",
    "        df_final = pd.merge(df_final, df_target_close, on=column_date, how='inner')\n",
    "        df_final['direction'] =df_final['close'] + (df_final['max'] - df_final['close']  + df_final['min'] - df_final['close'] + df_final['target_close'] - df_final['close'])/3\n",
    "        df_final = df_final[['date','direction']]\n",
    "        return(df_final)\n",
    "       \n",
    "\n",
    "###################################\n",
    "\n",
    "###  PARAMETERS  ###\n",
    "column_date='date'\n",
    "capas=5  # OJO! Hay que meterlo a mano\n",
    "epochs=45\n",
    "look_back = 40\n",
    "look_forward = 0\n",
    "batch_size = 20\n",
    "split = 0.1\n",
    "drop = 0.007\n",
    "optimizador = 'adam'\n",
    "initial_days=3\n",
    "days_target=10\n",
    "method_target = max\n",
    "method_target_to_share = str(method_target)[-4:-1]\n",
    "column_target= 'high'\n",
    "units = 200\n",
    "\n",
    "###################################\n",
    "\n",
    "def get_data_yfinance(asset, interval='1d', start='2011-01-01'):\n",
    "    date_column = 'Date'\n",
    "    list_of_days = ['1d', '5d', '1wk', '1mo', '3mo']\n",
    "    list_of_minutes = ['1m', '2m', '5m', '15m', '30m', '60m', '90m']\n",
    "    df = yf.download(asset, start=start, threads= False, interval=interval)\n",
    "    df = df.reset_index()\n",
    "    if interval== '1h':\n",
    "        df = df.rename(columns={'index':'Date'})\n",
    "    elif interval in list_of_minutes:\n",
    "        df = df.rename(columns={'Datetime':'Date'})\n",
    "        #df[date_column] = df[date_column].str[:]\n",
    "       \n",
    "    # Now that we have loaded our data into the dataframe, we can preview it using the print & .head() function\n",
    "   \n",
    "    df[date_column] = pd.to_datetime(df[date_column])\n",
    "    df = df.sort_values([date_column], ascending=[True]).reset_index(drop=True).sort_values([date_column], ascending=[False])\n",
    "    df = df.rename(columns={'Date':'date','Open':'open', 'High':'high', 'Low': 'low','Close':'close', 'Volume':'tradecount'})\n",
    "    return(df)\n",
    "\n",
    "###################################\n",
    "\n",
    "df = get_data_yfinance('BTC-USD')\n",
    "df = df[['date','open','high','low','close']]\n",
    "df_direction = target(df).direction()\n",
    "df = pd.merge(df,df_direction, on='date', how='inner')\n",
    "df['difference'] = (df['direction'] - df['close'])/df['close']\n",
    "df['target_class']=df[\"difference\"].apply(lambda x: 1 if x>0. else 0)\n",
    "#df = df[['date','open','high','low','close', 'target_class']]\n",
    "df_target = df[['date','target_class']]\n",
    "\n",
    "###################################\n",
    "\n",
    "# data preparation\n",
    "\n",
    "list_of_fetures_df = [df_CPTRA,\\\n",
    "df_DIFF,\\\n",
    "df_ETRVU,\\\n",
    "df_HRATE,\\\n",
    "df_MIREV,\\\n",
    "df_MKTCP,\\\n",
    "df_MWNTD,\\\n",
    "df_MWNUS,\\\n",
    "df_NADDU,\\\n",
    "df_NTRAN,\\\n",
    "df_NTRAT,\\\n",
    "df_NTRBL,\\\n",
    "df_NTREP,\\\n",
    "df_TOTBC,\\\n",
    "df_TRFUS,\\\n",
    "df_TRVOU]\n",
    "\n",
    "list_of_names_df = ['CPTRA',\\\n",
    "'DIFF',\\\n",
    "'ETRVU',\\\n",
    "'HRATE',\\\n",
    "'MIREV',\\\n",
    "'MKTCP',\\\n",
    "'MWNTD',\\\n",
    "'MWNUS',\\\n",
    "'NADDU',\\\n",
    "'NTRAN',\\\n",
    "'NTRAT',\\\n",
    "'NTRBL',\\\n",
    "'NTREP',\\\n",
    "'TOTBC',\\\n",
    "'TRFUS',\\\n",
    "'TRVOU']\n",
    "\n",
    "\n",
    "###################################\n",
    "\n",
    "df_final = df_target.copy()\n",
    "zipped_corr_list = list(zip(list_of_fetures_df, list_of_names_df))\n",
    "for i, j in zipped_corr_list:\n",
    "    i = i.reset_index().rename(columns={'Date':'date', 'Value':j})\n",
    "    df_final = pd.merge(df_final,i, on='date', how='inner')\n",
    "df_rf = df_final.drop(columns=['date'], axis=1)\n",
    "\n",
    "\n",
    "###################################\n",
    "\n",
    "df_final.to_csv('datos_hash.csv', encoding='Latin', sep=';', decimal=',')\n",
    "df.to_csv('bitcoin_price.csv', encoding='Latin', sep=';', decimal=',')\n",
    "\n",
    "###################################\n",
    "\n",
    "# modelos de clasificacion\n",
    "\n",
    "#X son todas las columnas que entrenarán el algoritmo salvo la clase a predecir, que no nos interesa que esté\n",
    "X=df_rf.drop(columns=['target_class'], axis=1)\n",
    "#Y será la clase a predecir\n",
    "Y=df_rf['target_class']\n",
    "\n",
    "#Spliteamos la secuencia en un 20% de testeo y un 80% para entrenar el modelo\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2)\n",
    "\n",
    "test = df_rf[-200:]\n",
    "train = df_rf[0:-200]\n",
    "X_train = train.drop(columns=['target_class'], axis=1)\n",
    "y_train = train['target_class']\n",
    "\n",
    "X_test = test.drop(columns=['target_class'], axis=1)\n",
    "y_test = test['target_class']\n",
    "\n",
    "rf = RandomForestClassifier( #class_weight={0:0.09,1:0.91},\n",
    "                             max_features='sqrt',\n",
    "                             max_depth=5,  \n",
    "                             min_samples_leaf=200,\n",
    "                             #min_samples_split=30,\n",
    "                             n_estimators=500,\n",
    "                             #random_state=100,\n",
    "                             #n_jobs=-1,\n",
    "                             #bootstrap=True,\n",
    "                             #criterion='gini'\n",
    "                            )  \n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "#Entrenamos el modelo con los datos de entrenamiento y las etiquetas o clases de entrenamiento\n",
    "rf.fit(X_train,y_train)\n",
    "#Usamos el modelo entrenado para predecir los datos de test. si hacemos el print(y_pred_rf, y_test), podemos ver el valor predicho frente al real.\n",
    "y_pred_rf=rf.predict(X_test)\n",
    "#esta es la matriz de probabilidades de test. Para un dato de entrada, cual es la probabilidad de que pertenezca a la clase 0, clase 1 y clase 2\n",
    "\n",
    "precision_score(y_test, y_pred_rf, average=None)\n",
    "precision = precision_score(y_test, y_pred_rf, average='micro')\n",
    "f1 = f1_score(y_test, y_pred_rf, average='micro')\n",
    "accuracy = accuracy_score(y_test, y_pred_rf)\n",
    "recall = recall_score(y_test, y_pred_rf)\n",
    "f1\n",
    "#En el aprtado en el que codificamos la clase, indicabamos que Neutral=1, Promotor=2 y por tanto Detractor=0\n",
    "target_names = ['class 0 - DOWN', 'class 1 - UP',]\n",
    "print(classification_report(y_test, y_pred_rf, target_names=target_names))\n",
    "confusion_matrix(y_test, y_pred_rf)\n",
    "feature_columns=list(X_train.columns)\n",
    "features=train[feature_columns]\n",
    "feature_list=train.columns.drop(['target_class'])\n",
    "\n",
    "importances = list(rf.feature_importances_)\n",
    "\n",
    "# List of tuples with variable and importance\n",
    "feature_importances = [(feature, round(importance, 4)) for feature, importance in zip(feature_list, importances)]\n",
    "\n",
    "# Sort the feature importances by most important first\n",
    "feature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n",
    "\n",
    "# Print out the feature and importances\n",
    "for pair in feature_importances:\n",
    "    print('Variable: {:20} Importance: {}'.format(*pair))\n",
    "\n",
    "###################################\n",
    "\n",
    "# modelos de probabilidad\n",
    "\n",
    "ypred_rf_proba=rf.predict_proba(X_test)\n",
    "ypred_rf_prob=ypred_rf_proba[:,1]\n",
    "auc = metrics.roc_auc_score(y_test, ypred_rf_prob)\n",
    "auc\n",
    "fpr, tpr, threshold = metrics.roc_curve(y_test, ypred_rf_prob)\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "plt.title('ROC Curve')\n",
    "plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()\n",
    "\n",
    "###################################\n",
    "\n",
    "# modelos de probabilidad\n",
    "\n",
    "ypred_rf_proba=rf.predict_proba(X_test)\n",
    "ypred_rf_prob=ypred_rf_proba[:,1]\n",
    "auc = metrics.roc_auc_score(y_test, ypred_rf_prob)\n",
    "auc\n",
    "fpr, tpr, threshold = metrics.roc_curve(y_test, ypred_rf_prob)\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "plt.title('ROC Curve')\n",
    "plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()\n",
    "#prueba=pd.concat(ypred_rf_prob,y_test)\n",
    "type(y_test)\n",
    "y_prob = pd.DataFrame(np.array(ypred_rf_prob))\n",
    "Y_test = pd.DataFrame(np.array(y_test))\n",
    "y_total=pd.merge(y_prob, Y_test, left_index=True, right_index=True)\n",
    "y_total = y_total.rename(columns={'0_x': 'y_pred_prob', '0_y': 'y_test'})\n",
    "y_ordenado=y_total.sort_values(by=['y_pred_prob'], ascending=[False])\n",
    "#y_por_ciento=y_ordenado[0:2000]\n",
    "\n",
    "# 1 Calcular tasa de churn Número de filas con label = 1 / número de filas del dataset\n",
    "\n",
    "pumpDataset=len(y_ordenado[y_ordenado.y_test == 1.0])/len(y_ordenado)*100\n",
    "\n",
    "# 2 Mediante un bucle, recorrer todo el dataset\n",
    "pumpDataset=len(y_ordenado[y_ordenado.y_test == 1.0])/len(y_ordenado)*100\n",
    "lift=[]\n",
    "x=[]\n",
    "for i in range(1,101):\n",
    "    x.append(i)\n",
    "    numeroDividir=int(len(y_ordenado)*(float(i)/100.0))\n",
    "    datasetTemporal=y_ordenado[0:numeroDividir]\n",
    "    pumpDatasetTemporal=len(datasetTemporal[datasetTemporal.y_test == 1.0])/len(datasetTemporal)*100\n",
    "    tasaLift=pumpDatasetTemporal/pumpDataset\n",
    "    lift.append(tasaLift)\n",
    "plt.plot(x, lift)\n",
    "plt.xlabel('Lift curve')\n",
    "plt.ylabel('UP detection rate Subtotal vs total')\n",
    "plt.show()\n",
    "\n",
    "###################################\n",
    "\n",
    "# convertimos indicadores a porcentajes\n",
    "\n",
    "\n",
    "list_of_indicators = [\n",
    "    df_CPTRA,\\\n",
    "    df_DIFF,\\\n",
    "    df_MIREV,\\\n",
    "    df_MWNUS,\\\n",
    "    df_NTRAT,\\\n",
    "    df_TOTBC\n",
    "]\n",
    "\n",
    "list_of_oscilators = [\n",
    "    df_ETRVU,\\\n",
    "    df_HRATE,\\\n",
    "    df_MWNTD,\\\n",
    "    df_MKTCP,\\\n",
    "    df_NADDU,\\\n",
    "    df_NTRAN,\\\n",
    "    df_NTRBL,\\\n",
    "    df_NTREP,\\\n",
    "    df_TRFUS,\\\n",
    "    df_TRVOU\n",
    "]\n",
    "\n",
    "list_of_name_indicators = [\n",
    "    'CPTRA',\\\n",
    "    'DIFF',\\\n",
    "    'MIREV',\\\n",
    "    'MWNUS',\\\n",
    "    'NTRAT',\\\n",
    "    'TOTBC',\n",
    "]\n",
    "\n",
    "list_of_name_oscilators = [\n",
    "    'ETRVU',\\\n",
    "    'HRATE',\\\n",
    "    'MWNTD',\\\n",
    "    'MKTCP',\\\n",
    "    'NADDU',\\\n",
    "    'NTRAN',\\\n",
    "    'NTRBL',\\\n",
    "    'NTREP',\\\n",
    "    'TRFUS',\\\n",
    "    'TRVOU',\n",
    "]\n",
    "\n",
    "def agregate_dataframes(df_target_new, list_of_fetures_df,list_of_names_df, date_column_df='Date', \\\n",
    "                        date_column_target='date',close_column_target='close', target_column='target_class'):\n",
    "    df_final = df_target_new.copy()\n",
    "    zipped_corr_list = list(zip(list_of_fetures_df, list_of_names_df))\n",
    "    for i, j in zipped_corr_list:\n",
    "        i = i.reset_index().rename(columns={date_column_df:date_column_target, 'Value':j})\n",
    "        df_final = pd.merge(df_final,i, on=date_column_target, how='inner')\n",
    "    df_rf = df_final.copy()\n",
    "    return(df_rf)\n",
    "\n",
    "def create_indicators_oscilators(list_of_indicators, \\\n",
    "                                 list_of_oscilators, \\\n",
    "                                 list_of_name_indicators, \\\n",
    "                                 df, \\\n",
    "                                 close_column_target='close', \\\n",
    "                                 date_column_indicators='Date', \\\n",
    "                                 date_column_target='date', \\\n",
    "                                 target_column='target_class'):\n",
    "   \n",
    "    df_new_target = df[[date_column_target, close_column_target]].copy()\n",
    "    df_indicators = agregate_dataframes(df_new_target, list_of_indicators, list_of_name_indicators)\n",
    "    list_of_new_features = []\n",
    "    for i in list_of_name_indicators:\n",
    "        new_feature = 'ind_'+i\n",
    "        df_indicators[new_feature] = df_indicators[i]/df_indicators[close_column_target]\n",
    "        list_of_new_features.append(new_feature)\n",
    "   \n",
    "    df_indicators = df_indicators.drop(columns=list_of_name_indicators, axis=1).drop(columns=close_column_target, axis=1)\n",
    "    #df_indicators = df_indicators.drop(columns=close_column_target, axis=1)\n",
    "    df_oscilators = agregate_dataframes(df_new_target, list_of_oscilators, list_of_name_oscilators).drop(columns=close_column_target, axis=1)\n",
    "    df_result = pd.merge(df_indicators,df_oscilators, on=date_column_target, how='inner')\n",
    "\n",
    "   \n",
    "    return(df_result)\n",
    "df_indicators_oscilators = create_indicators_oscilators(list_of_indicators, list_of_oscilators, list_of_name_indicators, df)\n",
    "\n",
    "\n",
    "\n",
    "###################################\n",
    "\n",
    "# modelo biclase\n",
    "\n",
    "df_rf = pd.merge(df_target,df_indicators_oscilators, on='date', how='inner')\n",
    "df_rf = df_rf.drop(columns=['date'], axis=1)\n",
    "\n",
    "# Compute the correlation matrix\n",
    "corr = df_rf.corr()\n",
    "\n",
    "# Generate a mask for the upper triangle\n",
    "mask = np.zeros_like(corr, dtype=np.bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "f, ax = plt.subplots(figsize=(20, 18))\n",
    "\n",
    "# Generate a custom diverging colormap\n",
    "cmap = sns.diverging_palette(120, 10, as_cmap=True)\n",
    "\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n",
    "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n",
    "\n",
    "#X son todas las columnas que entrenarán el algoritmo salvo la clase a predecir, que no nos interesa que esté\n",
    "X=df_rf.drop(columns=['target_class'], axis=1)\n",
    "#Y será la clase a predecir\n",
    "Y=df_rf['target_class']\n",
    "\n",
    "#Spliteamos la secuencia en un 20% de testeo y un 80% para entrenar el modelo\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2)\n",
    "\n",
    "test = df_rf[-200:]\n",
    "train = df_rf[0:-200]\n",
    "X_train = train.drop(columns=['target_class'], axis=1)\n",
    "y_train = train['target_class']\n",
    "\n",
    "X_test = test.drop(columns=['target_class'], axis=1)\n",
    "y_test = test['target_class']\n",
    "rf = RandomForestClassifier( #class_weight={0:0.09,1:0.91},\n",
    "                             max_features='sqrt',\n",
    "                             max_depth=5,  \n",
    "                             min_samples_leaf=200,\n",
    "                             #min_samples_split=30,\n",
    "                             n_estimators=500,\n",
    "                             #random_state=100,\n",
    "                             #n_jobs=-1,\n",
    "                             #bootstrap=True,\n",
    "                             #criterion='gini'\n",
    "                            )  \n",
    "rf = RandomForestClassifier()\n",
    "rf = RandomForestClassifier( #class_weight={0:0.09,1:0.91},\n",
    "                             max_features=None,\n",
    "                             max_depth=15,  \n",
    "                             min_samples_leaf=20,\n",
    "                             min_samples_split=2,\n",
    "                             n_estimators=400,\n",
    "                             #random_state=100,\n",
    "                             #n_jobs=-1,\n",
    "                             #bootstrap=True,\n",
    "                             #criterion='gini'\n",
    "                            )  \n",
    "\n",
    "#Entrenamos el modelo con los datos de entrenamiento y las etiquetas o clases de entrenamiento\n",
    "rf.fit(X_train,y_train)\n",
    "#Usamos el modelo entrenado para predecir los datos de test. si hacemos el print(y_pred_rf, y_test), podemos ver el valor predicho frente al real.\n",
    "y_pred_rf=rf.predict(X_test)\n",
    "#esta es la matriz de probabilidades de test. Para un dato de entrada, cual es la probabilidad de que pertenezca a la clase 0, clase 1 y clase 2\n",
    "precision_score(y_test, y_pred_rf, average=None)\n",
    "\n",
    "precision_score(y_test, y_pred_rf, average='micro')\n",
    "#En el aprtado en el que codificamos la clase, indicabamos que Neutral=1, Promotor=2 y por tanto Detractor=0\n",
    "target_names = ['class 0 - DOWN', 'class 1 - UP',]\n",
    "print(classification_report(y_test, y_pred_rf, target_names=target_names))\n",
    "confusion_matrix(y_test, y_pred_rf)\n",
    "\n",
    "feature_columns=list(X_train.columns)\n",
    "features=train[feature_columns]\n",
    "feature_list=train.columns.drop(['target_class'])\n",
    "\n",
    "importances = list(rf.feature_importances_)\n",
    "\n",
    "# List of tuples with variable and importance\n",
    "feature_importances = [(feature, round(importance, 4)) for feature, importance in zip(feature_list, importances)]\n",
    "\n",
    "# Sort the feature importances by most important first\n",
    "feature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n",
    "\n",
    "# Print out the feature and importances\n",
    "for pair in feature_importances:\n",
    "    print('Variable: {:20} Importance: {}'.format(*pair))\n",
    "\n",
    "   \n",
    "###################################\n",
    "\n",
    "# modelo de probabilidad\n",
    "\n",
    "ypred_rf_proba=rf.predict_proba(X_test)\n",
    "ypred_rf_proba\n",
    "ypred_rf_prob=ypred_rf_proba[:,1]\n",
    "metrics.roc_auc_score(y_test, ypred_rf_prob)\n",
    "fpr, tpr, threshold = metrics.roc_curve(y_test, ypred_rf_prob)\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "plt.title('ROC Curve')\n",
    "plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()\n",
    "#prueba=pd.concat(ypred_rf_prob,y_test)\n",
    "type(y_test)\n",
    "y_prob = pd.DataFrame(np.array(ypred_rf_prob))\n",
    "Y_test = pd.DataFrame(np.array(y_test))\n",
    "y_total=pd.merge(y_prob, Y_test, left_index=True, right_index=True)\n",
    "y_total = y_total.rename(columns={'0_x': 'y_pred_prob', '0_y': 'y_test'})\n",
    "y_ordenado=y_total.sort_values(by=['y_pred_prob'], ascending=[False])\n",
    "#y_por_ciento=y_ordenado[0:2000]\n",
    "\n",
    "# 1 Calcular tasa de churn Número de filas con label = 1 / número de filas del dataset\n",
    "\n",
    "pumpDataset=len(y_ordenado[y_ordenado.y_test == 1.0])/len(y_ordenado)*100\n",
    "\n",
    "# 2 Mediante un bucle, recorrer todo el dataset\n",
    "pumpDataset=len(y_ordenado[y_ordenado.y_test == 1.0])/len(y_ordenado)*100\n",
    "lift=[]\n",
    "x=[]\n",
    "for i in range(1,101):\n",
    "    x.append(i)\n",
    "    numeroDividir=int(len(y_ordenado)*(float(i)/100.0))\n",
    "    datasetTemporal=y_ordenado[0:numeroDividir]\n",
    "    pumpDatasetTemporal=len(datasetTemporal[datasetTemporal.y_test == 1.0])/len(datasetTemporal)*100\n",
    "    tasaLift=pumpDatasetTemporal/pumpDataset\n",
    "    lift.append(tasaLift)\n",
    "plt.plot(x, lift)\n",
    "plt.xlabel('Lift curve')\n",
    "plt.ylabel('UP detection rate Subtotal vs total')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "########################################\n",
    "\n",
    "# RANDOM FOREST GRID SEARCH\n",
    "\n",
    "rfGS = RandomForestClassifier(n_jobs=-1)\n",
    "param_grid = {\n",
    "    'max_features':['sqrt','log2',None],\n",
    "    'n_estimators':[100,1000],  \n",
    "    'min_samples_split':[2,4,8],    \n",
    "    'max_depth':[5,10,None],      \n",
    "}\n",
    "\n",
    "gs = GridSearchCV(rfGS, param_grid, cv=4, scoring='f1_micro')#,  scoring='f1')\n",
    "gs.fit(X_train, y_train)\n",
    "best_model_GSRF = gs.best_estimator_\n",
    "\n",
    "\n",
    "print('Best Params: ',gs.best_params_)\n",
    "\n",
    "rf = RandomForestClassifier( #class_weight={0:0.09,1:0.91},\n",
    "                             max_features=None,\n",
    "                             max_depth=15,  \n",
    "                             min_samples_leaf=20,\n",
    "                             min_samples_split=2,\n",
    "                             n_estimators=400,\n",
    "                             #random_state=100,\n",
    "                             #n_jobs=-1,\n",
    "                             #bootstrap=True,\n",
    "                             #criterion='gini'\n",
    "                            )  \n",
    "\n",
    "#Entrenamos el modelo con los datos de entrenamiento y las etiquetas o clases de entrenamiento\n",
    "rf.fit(X_train,y_train)\n",
    "#Usamos el modelo entrenado para predecir los datos de test. si hacemos el print(y_pred_rf, y_test), podemos ver el valor predicho frente al real.\n",
    "y_pred_rf=rf.predict(X_test)\n",
    "#esta es la matriz de probabilidades de test. Para un dato de entrada, cual es la probabilidad de que pertenezca a la clase 0, clase 1 y clase 2\n",
    "\n",
    "precision_score(y_test, y_pred_rf, average=None)\n",
    "\n",
    "precision_score(y_test, y_pred_rf, average='micro')\n",
    "\n",
    "#En el aprtado en el que codificamos la clase, indicabamos que Neutral=1, Promotor=2 y por tanto Detractor=0\n",
    "target_names = ['class 0 - DOWN', 'class 1 - UP',]\n",
    "print(classification_report(y_test, y_pred_rf, target_names=target_names))\n",
    "confusion_matrix(y_test, y_pred_rf)\n",
    "\n",
    "\n",
    "####################################\n",
    "\n",
    "# MOST CORRELATED VARIABLES\n",
    "\n",
    "df_final = df_target.copy()\n",
    "zipped_corr_list = list(zip(list_of_fetures_df, list_of_names_df))\n",
    "for i, j in zipped_corr_list:\n",
    "    i = i.reset_index().rename(columns={'Date':'date', 'Value':j})\n",
    "    df_final = pd.merge(df_final,i, on='date', how='inner')\n",
    "\n",
    "\n",
    "list_most_corr = ['MKTCP',\\\n",
    "'MWNUS',\\\n",
    "'TOTBC',\\\n",
    "'NTRAT',\\\n",
    "'HRATE',\\\n",
    "'NTREP',\\\n",
    "'TRFUS',\\\n",
    "'MWNTD']\n",
    "\n",
    "list_most_corr += ['date','target_class']\n",
    "df_final = df_final[list_most_corr]\n",
    "df_rf = df_final.drop(columns=['date'], axis=1)\n",
    "#X son todas las columnas que entrenarán el algoritmo salvo la clase a predecir, que no nos interesa que esté\n",
    "X=df_rf.drop(columns=['target_class'], axis=1)\n",
    "#Y será la clase a predecir\n",
    "Y=df_rf['target_class']\n",
    "\n",
    "#Spliteamos la secuencia en un 20% de testeo y un 80% para entrenar el modelo\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2)\n",
    "\n",
    "test = df_rf[-200:]\n",
    "train = df_rf[0:-200]\n",
    "X_train = train.drop(columns=['target_class'], axis=1)\n",
    "y_train = train['target_class']\n",
    "\n",
    "X_test = test.drop(columns=['target_class'], axis=1)\n",
    "y_test = test['target_class']\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "#Entrenamos el modelo con los datos de entrenamiento y las etiquetas o clases de entrenamiento\n",
    "rf.fit(X_train,y_train)\n",
    "#Usamos el modelo entrenado para predecir los datos de test. si hacemos el print(y_pred_rf, y_test), podemos ver el valor predicho frente al real.\n",
    "y_pred_rf=rf.predict(X_test)\n",
    "#esta es la matriz de probabilidades de test. Para un dato de entrada, cual es la probabilidad de que pertenezca a la clase 0, clase 1 y clase 2\n",
    "ypred_rf_proba=rf.predict_proba(X_test)\n",
    "ypred_rf_proba\n",
    "ypred_rf_prob=ypred_rf_proba[:,1]\n",
    "metrics.roc_auc_score(y_test, ypred_rf_prob)\n",
    "fpr, tpr, threshold = metrics.roc_curve(y_test, ypred_rf_prob)\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "plt.title('ROC Curve')\n",
    "plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()\n",
    "#prueba=pd.concat(ypred_rf_prob,y_test)\n",
    "type(y_test)\n",
    "y_prob = pd.DataFrame(np.array(ypred_rf_prob))\n",
    "Y_test = pd.DataFrame(np.array(y_test))\n",
    "y_total=pd.merge(y_prob, Y_test, left_index=True, right_index=True)\n",
    "y_total = y_total.rename(columns={'0_x': 'y_pred_prob', '0_y': 'y_test'})\n",
    "y_ordenado=y_total.sort_values(by=['y_pred_prob'], ascending=[False])\n",
    "y_por_ciento=y_ordenado[0:2000]\n",
    "\n",
    "# 1 Calcular tasa de churn Número de filas con label = 1 / número de filas del dataset\n",
    "\n",
    "pumpDataset=len(y_ordenado[y_ordenado.y_test == 1.0])/len(y_ordenado)*100\n",
    "\n",
    "# 2 Mediante un bucle, recorrer todo el dataset\n",
    "pumpDataset=len(y_ordenado[y_ordenado.y_test == 1.0])/len(y_ordenado)*100\n",
    "lift=[]\n",
    "x=[]\n",
    "for i in range(1,101):\n",
    "    x.append(i)\n",
    "    numeroDividir=int(len(y_ordenado)*(float(i)/100.0))\n",
    "    datasetTemporal=y_ordenado[0:numeroDividir]\n",
    "    pumpDatasetTemporal=len(datasetTemporal[datasetTemporal.y_test == 1.0])/len(datasetTemporal)*100\n",
    "    tasaLift=pumpDatasetTemporal/pumpDataset\n",
    "    lift.append(tasaLift)\n",
    "plt.plot(x, lift)\n",
    "plt.xlabel('Lift curve')\n",
    "plt.ylabel('UP detection rate Subtotal vs total')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "####################################\n",
    "\n",
    "# WITH INCREMENTAL\n",
    "\n",
    "def incremental_calculation(df, days, column_date='date' ):\n",
    "    df_previous = df.copy(deep=True)\n",
    "    list_of_features=[]\n",
    "    list_of_features_to_delete = []\n",
    "    for column in list(df_previous.columns):\n",
    "        if ((\"area\" in column) or (\"CPR\" in column) or (\"fibo\" in column) or (\"target\" in column)):\n",
    "            list_of_features_to_delete.append(column)\n",
    "        else:\n",
    "            list_of_features.append(column)\n",
    "    df_previous['date_join'] = df_previous[column_date] + pd.Timedelta(days=+days)\n",
    "    df_previous = df_previous.drop(columns=[column_date], axis=1)\n",
    "    dictionary={}\n",
    "    for i in df_previous.columns:\n",
    "        a = str(i)\n",
    "        b = str(i) + ('_previous')\n",
    "        dictionary[a]=b\n",
    "    df_previous = df_previous.rename(dictionary, axis=1)\n",
    "    # drop supports and resistances\n",
    "    list_of_features_previous=[]\n",
    "    list_of_features_previous_to_delete=[]\n",
    "    for column in list(df_previous.columns):\n",
    "        if ((\"area\" in column) or (\"CPR\" in column) or (\"fibo\" in column) or (\"target\" in column)):\n",
    "            list_of_features_previous_to_delete.append(column)\n",
    "        else:\n",
    "            list_of_features_previous.append(column)\n",
    "    df_previous = df_previous[list_of_features_previous]\n",
    "    df_joined = pd.merge(df,df_previous, how='left', left_on=column_date, right_on=column_date+'_join_previous')\n",
    "    #Calulation of incrementals\n",
    "    final_list_of_features = [column_date+'_join_previous']\n",
    "    for column in list_of_features:\n",
    "        try:\n",
    "            df_joined[column+'_inc_'+ str(days) +'_days'] = df_joined[column]-df_joined[column+'_previous']\n",
    "            final_list_of_features.append(column+'_inc_'+ str(days) +'_days')\n",
    "        except:\n",
    "            pass\n",
    "    #df_joined['close'+'_inc_'+ str(days) +'_days'] = 100*(df_joined['close']-df_joined['close'+'_previous'])/df_joined['close']\n",
    "    df_joined = df_joined[final_list_of_features].dropna()\n",
    "    return(df_joined)\n",
    "\n",
    "df_final = df_target.copy()\n",
    "zipped_corr_list = list(zip(list_of_fetures_df, list_of_names_df))\n",
    "for i, j in zipped_corr_list:\n",
    "    i = i.reset_index().rename(columns={'Date':'date', 'Value':j})\n",
    "    df_final = pd.merge(df_final,i, on='date', how='inner')\n",
    "df_inc3 = incremental_calculation(df_final, 3)\n",
    "df_inc7 = incremental_calculation(df_final, 7)\n",
    "df_inc = pd.merge(df_inc3,df_inc7, on='date_join_previous', how='inner')\n",
    "df_final_inc = pd.merge(df_final, df_inc, left_on='date', right_on='date_join_previous', how='inner')\n",
    "df_rf = df_final_inc.drop(columns=['date', 'date_join_previous'], axis=1)\n",
    "#X son todas las columnas que entrenarán el algoritmo salvo la clase a predecir, que no nos interesa que esté\n",
    "X=df_rf.drop(columns=['target_class'], axis=1)\n",
    "#Y será la clase a predecir\n",
    "Y=df_rf['target_class']\n",
    "#Spliteamos la secuencia en un 20% de testeo y un 80% para entrenar el modelo\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2)\n",
    "\n",
    "test = df_rf[-100:]\n",
    "train = df_rf[0:-100]\n",
    "X_train = train.drop(columns=['target_class'], axis=1)\n",
    "y_train = train['target_class']\n",
    "\n",
    "X_test = test.drop(columns=['target_class'], axis=1)\n",
    "y_test = test['target_class']\n",
    "#Entrenamos el modelo con los datos de entrenamiento y las etiquetas o clases de entrenamiento\n",
    "rf.fit(X_train,y_train)\n",
    "#Usamos el modelo entrenado para predecir los datos de test. si hacemos el print(y_pred_rf, y_test), podemos ver el valor predicho frente al real.\n",
    "y_pred_rf=rf.predict(X_test)\n",
    "#esta es la matriz de probabilidades de test. Para un dato de entrada, cual es la probabilidad de que pertenezca a la clase 0, clase 1 y clase 2\n",
    "feature_columns=list(X_train.columns)\n",
    "features=train[feature_columns]\n",
    "feature_list=train.columns.drop(['target_class'])\n",
    "\n",
    "importances = list(rf.feature_importances_)\n",
    "\n",
    "# List of tuples with variable and importance\n",
    "feature_importances = [(feature, round(importance, 4)) for feature, importance in zip(feature_list, importances)]\n",
    "\n",
    "# Sort the feature importances by most important first\n",
    "feature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n",
    "\n",
    "# Print out the feature and importances\n",
    "for pair in feature_importances:\n",
    "    print('Variable: {:20} Importance: {}'.format(*pair))\n",
    "   \n",
    "ypred_rf_proba=rf.predict_proba(X_test)\n",
    "ypred_rf_proba\n",
    "ypred_rf_prob=ypred_rf_proba[:,1]\n",
    "\n",
    "metrics.roc_auc_score(y_test, ypred_rf_prob)\n",
    "fpr, tpr, threshold = metrics.roc_curve(y_test, ypred_rf_prob)\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.title('ROC Curve')\n",
    "plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()\n",
    "#prueba=pd.concat(ypred_rf_prob,y_test)\n",
    "type(y_test)\n",
    "y_prob = pd.DataFrame(np.array(ypred_rf_prob))\n",
    "Y_test = pd.DataFrame(np.array(y_test))\n",
    "y_total=pd.merge(y_prob, Y_test, left_index=True, right_index=True)\n",
    "y_total = y_total.rename(columns={'0_x': 'y_pred_prob', '0_y': 'y_test'})\n",
    "y_ordenado=y_total.sort_values(by=['y_pred_prob'], ascending=[False])\n",
    "y_por_ciento=y_ordenado[0:2000]\n",
    "\n",
    "# 1 Calcular tasa de churn Número de filas con label = 1 / número de filas del dataset\n",
    "\n",
    "churnDataset=len(y_ordenado[y_ordenado.y_test == 1.0])/len(y_ordenado)*100\n",
    "\n",
    "# 2 Mediante un bucle, recorrer todo el dataset\n",
    "churnDataset=len(y_ordenado[y_ordenado.y_test == 1.0])/len(y_ordenado)*100\n",
    "lift=[]\n",
    "x=[]\n",
    "for i in range(1,101):\n",
    "    x.append(i)\n",
    "    numeroDividir=int(len(y_ordenado)*(float(i)/100.0))\n",
    "    datasetTemporal=y_ordenado[0:numeroDividir]\n",
    "    churnDatasetTemporal=len(datasetTemporal[datasetTemporal.y_test == 1.0])/len(datasetTemporal)*100\n",
    "    tasaLift=churnDatasetTemporal/churnDataset\n",
    "    lift.append(tasaLift)\n",
    "plt.plot(x, lift)\n",
    "plt.xlabel('Lift curve')\n",
    "plt.ylabel('UP detection rate Subtotal vs total')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "####################################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
